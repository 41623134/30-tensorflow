{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-3 activation\n",
    "\n",
    "Activation function plays a key role in deep learning. It introduces the nonlinearity that enables the neural network to fit arbitrary complicated functions.\n",
    "\n",
    "The neural network, no matter how complicated the structure is, is still a linear transformation which cannot fit the nonlinear functions without the activation function.\n",
    "\n",
    "For the time being, the most popular activation function is `relu`, but there are some new functions such as `swish`, `GELU`, claiming a better performance over `relu`.\n",
    "\n",
    "Here are two review papers to the activation function (in Chinese).\n",
    "\n",
    "[《一文概览深度学习中的激活函数》](https://zhuanlan.zhihu.com/p/98472075)\n",
    "\n",
    "[《从ReLU到GELU,一文概览神经网络中的激活函数》](https://zhuanlan.zhihu.com/p/98863801)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The most popular activation functions\n",
    "\n",
    "\n",
    "* `tf.nn.sigmoid`: Compressing real number between 0 to 1, usually used in the output layer for binary classification; the main drawbacks are vanishing gradient, high computing complexity, and the non-zero center of the output.\n",
    "\n",
    "![](./data/sigmoid.png)\n",
    "\n",
    "* `tf.nn.softmax`: Extended version of sigmoid for multiple categories, usually used in the output layer for multiple classifications.\n",
    "\n",
    "![](./data/softmax说明.jpg)\n",
    "\n",
    "* `tf.nn.tanh`：Compressing real number between -1 to 1, expectation of the output is zero; the main drawbacks are vanishing gradient and high computing complexity.\n",
    "\n",
    "![](./data/tanh.png)\n",
    "\n",
    "* `tf.nn.relu`：Linear rectified unit, the most popular activation function, usually used in the hidden layer; the main drawbacks are non-zero center of the output and vanishing gradient for the inputs < 0 (dying relu).\n",
    "\n",
    "![](./data/relu.png)\n",
    "\n",
    "* `tf.nn.leaky_relu`：Improved ReLU, resolving the dying ReLU problem.\n",
    "\n",
    "![](./data/leaky_relu.png)\n",
    "\n",
    "* `tf.nn.elu`：Exponential linear unit, which is an improvement to the ReLU, alleviate the dying ReLU problem.\n",
    "\n",
    "![](./data/elu.png)\n",
    "\n",
    "* `tf.nn.selu`：Scaled exponential linear unit, which is able to normalize the neural network automatically if the weights are initialized through `tf.keras.initializers.lecun_normal`. No gradient exploding/vanishing problems, but need to apply together with AlphaDropout (an alternation of Dropout).\n",
    "\n",
    "![](./data/selu.png)\n",
    "\n",
    "* `tf.nn.swish`：Self-gated activation function, a research product form Google. The literature prove that it brings slight improvement comparing to ReLU.\n",
    "\n",
    "![](./data/swish.png)\n",
    "\n",
    "* `gelu`：Gaussian error linear unit, which has the best performance in Transformer; however `tf.nn` hasn't implemented it.\n",
    "\n",
    "![](./data/gelu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing activation functions in the models\n",
    "\n",
    "\n",
    "There are two ways of implementing activation functions in Keras models: specifying through the `activation` parameter in certain layers, or adding activation layer `layers.Activation` explicitly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, None, 32)          544       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 10)          330       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, None, 10)          0         \n",
      "=================================================================\n",
      "Total params: 874\n",
      "Trainable params: 874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32,input_shape = (None,16),activation = tf.nn.relu)) # Specifying through the activation parameter\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Activation(tf.nn.softmax))  # adding `layers.Activation` explicitly.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
