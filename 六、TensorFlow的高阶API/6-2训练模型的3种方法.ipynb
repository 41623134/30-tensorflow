{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-2训练模型的3种方法\n",
    "\n",
    "模型的训练主要有内置fit方法、内置tran_on_batch方法、自定义训练循环。\n",
    "\n",
    "注：fit_generator方法在tf.keras中不推荐使用，其功能已经被fit包含。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import * \n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 9s 4us/step\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train), (x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max() + 1\n",
    "CAT_NUM = y_train.max() + 1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size=1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size=1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、内置fit方法\n",
    "\n",
    "该方法功能非常强大, 支持对numpy array, tf.data.Dataset以及 Python generator数据进行训练。\n",
    "\n",
    "并且可以通过设置回调函数实现对训练过程的复杂控制逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 281 steps, validate for 71 steps\n",
      "Epoch 1/10\n",
      "281/281 [==============================] - 5s 18ms/step - loss: 2.0142 - sparse_categorical_accuracy: 0.4727 - sparse_top_k_categorical_accuracy: 0.7448 - val_loss: 1.6425 - val_sparse_categorical_accuracy: 0.5766 - val_sparse_top_k_categorical_accuracy: 0.7609\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 4s 15ms/step - loss: 1.4469 - sparse_categorical_accuracy: 0.6296 - sparse_top_k_categorical_accuracy: 0.8033 - val_loss: 1.4753 - val_sparse_categorical_accuracy: 0.6322 - val_sparse_top_k_categorical_accuracy: 0.7983\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 4s 16ms/step - loss: 1.1427 - sparse_categorical_accuracy: 0.6988 - sparse_top_k_categorical_accuracy: 0.8667 - val_loss: 1.4703 - val_sparse_categorical_accuracy: 0.6429 - val_sparse_top_k_categorical_accuracy: 0.8206\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 5s 16ms/step - loss: 0.8686 - sparse_categorical_accuracy: 0.7751 - sparse_top_k_categorical_accuracy: 0.9211 - val_loss: 1.6168 - val_sparse_categorical_accuracy: 0.6398 - val_sparse_top_k_categorical_accuracy: 0.8166\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 4s 16ms/step - loss: 0.6437 - sparse_categorical_accuracy: 0.8368 - sparse_top_k_categorical_accuracy: 0.9540 - val_loss: 1.8477 - val_sparse_categorical_accuracy: 0.6380 - val_sparse_top_k_categorical_accuracy: 0.8134\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 4s 16ms/step - loss: 0.4920 - sparse_categorical_accuracy: 0.8805 - sparse_top_k_categorical_accuracy: 0.9713 - val_loss: 2.1294 - val_sparse_categorical_accuracy: 0.6340 - val_sparse_top_k_categorical_accuracy: 0.8117\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 4s 15ms/step - loss: 0.3974 - sparse_categorical_accuracy: 0.9049 - sparse_top_k_categorical_accuracy: 0.9812 - val_loss: 2.3764 - val_sparse_categorical_accuracy: 0.6318 - val_sparse_top_k_categorical_accuracy: 0.8099\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 4s 16ms/step - loss: 0.3368 - sparse_categorical_accuracy: 0.9204 - sparse_top_k_categorical_accuracy: 0.9865 - val_loss: 2.5689 - val_sparse_categorical_accuracy: 0.6256 - val_sparse_top_k_categorical_accuracy: 0.8121\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 5s 16ms/step - loss: 0.2962 - sparse_categorical_accuracy: 0.9290 - sparse_top_k_categorical_accuracy: 0.9901 - val_loss: 2.7363 - val_sparse_categorical_accuracy: 0.6162 - val_sparse_top_k_categorical_accuracy: 0.8139\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 5s 16ms/step - loss: 0.2659 - sparse_categorical_accuracy: 0.9367 - sparse_top_k_categorical_accuracy: 0.9916 - val_loss: 2.8769 - val_sparse_categorical_accuracy: 0.6095 - val_sparse_top_k_categorical_accuracy: 0.8126\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train, validation_data=ds_test, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、内置train_on_batch方法\n",
    "\n",
    "该内置方法相比较fit方法更加灵活，可以不通过回调函数而直接在批次层次上更加精细地控制训练的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modle, ds_train, ds_valid, epoches):\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        modle.reset_metrics()\n",
    "        \n",
    "        # 在后期降低学习率\n",
    "        if epoch == 5:\n",
    "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
    "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
    "            \n",
    "        for x, y in ds_train:\n",
    "            train_result = model.train_on_batch(x, y)\n",
    "            \n",
    "        for x, y in ds_valid:\n",
    "            valid_result = modle.test_on_batch(x, y, reset_metrics=False)\n",
    "        \n",
    "        if epoch%1 == 0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch = \", epoch)\n",
    "            print(\"train:\", dict(zip(model.metrics_names, train_result)))\n",
    "            print(\"vaild:\", dict(zip(model.metrics_names, valid_result)))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================20:24:12\n",
      "epoch =  1\n",
      "train: {'loss': 1.3048908, 'sparse_categorical_accuracy': 0.6818182, 'sparse_top_k_categorical_accuracy': 0.8636364}\n",
      "vaild: {'loss': 1.1265022, 'sparse_categorical_accuracy': 0.56500447, 'sparse_top_k_categorical_accuracy': 0.76179874}\n",
      "\n",
      "================================================================================20:24:16\n",
      "epoch =  2\n",
      "train: {'loss': 0.981869, 'sparse_categorical_accuracy': 0.77272725, 'sparse_top_k_categorical_accuracy': 0.8181818}\n",
      "vaild: {'loss': 1.0809889, 'sparse_categorical_accuracy': 0.61353517, 'sparse_top_k_categorical_accuracy': 0.7911843}\n",
      "\n",
      "================================================================================20:24:20\n",
      "epoch =  3\n",
      "train: {'loss': 0.7001989, 'sparse_categorical_accuracy': 0.8181818, 'sparse_top_k_categorical_accuracy': 0.95454544}\n",
      "vaild: {'loss': 0.85464996, 'sparse_categorical_accuracy': 0.63891363, 'sparse_top_k_categorical_accuracy': 0.80142474}\n",
      "\n",
      "================================================================================20:24:24\n",
      "epoch =  4\n",
      "train: {'loss': 0.40070954, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "vaild: {'loss': 0.9760583, 'sparse_categorical_accuracy': 0.6362422, 'sparse_top_k_categorical_accuracy': 0.8045414}\n",
      "\n",
      "Lowering optimizer Learning Rate...\n",
      "\n",
      "\n",
      "================================================================================20:24:28\n",
      "epoch =  5\n",
      "train: {'loss': 0.22230266, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "vaild: {'loss': 1.2007524, 'sparse_categorical_accuracy': 0.6313446, 'sparse_top_k_categorical_accuracy': 0.80142474}\n",
      "\n",
      "================================================================================20:24:32\n",
      "epoch =  6\n",
      "train: {'loss': 0.1797568, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "vaild: {'loss': 1.368887, 'sparse_categorical_accuracy': 0.6277827, 'sparse_top_k_categorical_accuracy': 0.8045414}\n",
      "\n",
      "================================================================================20:24:37\n",
      "epoch =  7\n",
      "train: {'loss': 0.14951001, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "vaild: {'loss': 1.4589005, 'sparse_categorical_accuracy': 0.6273375, 'sparse_top_k_categorical_accuracy': 0.8067676}\n",
      "\n",
      "================================================================================20:24:42\n",
      "epoch =  8\n",
      "train: {'loss': 0.13020688, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "vaild: {'loss': 1.4636511, 'sparse_categorical_accuracy': 0.6237756, 'sparse_top_k_categorical_accuracy': 0.8103295}\n",
      "\n",
      "================================================================================20:24:46\n",
      "epoch =  9\n",
      "train: {'loss': 0.11717785, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "vaild: {'loss': 1.434085, 'sparse_categorical_accuracy': 0.61932325, 'sparse_top_k_categorical_accuracy': 0.8134461}\n",
      "\n",
      "================================================================================20:24:50\n",
      "epoch =  10\n",
      "train: {'loss': 0.108724736, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "vaild: {'loss': 1.419117, 'sparse_categorical_accuracy': 0.6170971, 'sparse_top_k_categorical_accuracy': 0.81121993}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、自定义训练循环\n",
    "\n",
    "自定义训练循环无需编译模型，直接利用优化器根据损失函数反向传播迭代参数，拥有最高的灵活性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================20:25:46\n",
      "Epoch=1,Loss:2.03816032,Accuracy:0.460810512,Valid Loss:1.67894149,Valid Accuracy:0.555209279\n",
      "\n",
      "================================================================================20:25:50\n",
      "Epoch=2,Loss:1.46901417,Accuracy:0.620017827,Valid Loss:1.50382268,Valid Accuracy:0.61576134\n",
      "\n",
      "================================================================================20:25:54\n",
      "Epoch=3,Loss:1.19141817,Accuracy:0.688376725,Valid Loss:1.51255655,Valid Accuracy:0.626447\n",
      "\n",
      "================================================================================20:25:58\n",
      "Epoch=4,Loss:0.945563614,Accuracy:0.753507,Valid Loss:1.65834796,Valid Accuracy:0.630008876\n",
      "\n",
      "================================================================================20:26:02\n",
      "Epoch=5,Loss:0.710940301,Accuracy:0.816633284,Valid Loss:1.90381527,Valid Accuracy:0.630454123\n",
      "\n",
      "================================================================================20:26:06\n",
      "Epoch=6,Loss:0.530909836,Accuracy:0.86951679,Valid Loss:2.21079826,Valid Accuracy:0.629563689\n",
      "\n",
      "================================================================================20:26:10\n",
      "Epoch=7,Loss:0.415897369,Accuracy:0.902137637,Valid Loss:2.48629808,Valid Accuracy:0.625556529\n",
      "\n",
      "================================================================================20:26:13\n",
      "Epoch=8,Loss:0.347128928,Accuracy:0.918281,Valid Loss:2.7009027,Valid Accuracy:0.61709708\n",
      "\n",
      "================================================================================20:26:17\n",
      "Epoch=9,Loss:0.298624367,Accuracy:0.928189695,Valid Loss:2.8735373,Valid Accuracy:0.607301891\n",
      "\n",
      "================================================================================20:26:21\n",
      "Epoch=10,Loss:0.264814645,Accuracy:0.935983062,Valid Loss:3.01624084,Valid Accuracy:0.595280528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
