{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-2 Three Ways of Training\n",
    "\n",
    "There are three ways of model training: using pre-defined `fit` method, using pre-defined `tran_on_batch` method, using customized training loop.\n",
    "\n",
    "Note: `fit_generator` method is not recommended in `tf.keras` since it has been merged into `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import * \n",
    "\n",
    "# Time stamps\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 3s 2us/step\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-defined `fit` method\n",
    "\n",
    "\n",
    "This is a powerful method, which supports training the data with types of numpy array, `tf.data.Dataset` and Python generator.\n",
    "\n",
    "This method also supports complicated logical controlling through proper configuration of the callbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "281/281 [==============================] - 14s 48ms/step - loss: 2.0285 - sparse_categorical_accuracy: 0.4713 - sparse_top_k_categorical_accuracy: 0.7466 - val_loss: 1.6899 - val_sparse_categorical_accuracy: 0.5641 - val_sparse_top_k_categorical_accuracy: 0.7569\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 14s 48ms/step - loss: 1.5068 - sparse_categorical_accuracy: 0.6070 - sparse_top_k_categorical_accuracy: 0.7920 - val_loss: 1.5585 - val_sparse_categorical_accuracy: 0.5975 - val_sparse_top_k_categorical_accuracy: 0.7881\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 13s 46ms/step - loss: 1.2200 - sparse_categorical_accuracy: 0.6794 - sparse_top_k_categorical_accuracy: 0.8454 - val_loss: 1.5464 - val_sparse_categorical_accuracy: 0.6331 - val_sparse_top_k_categorical_accuracy: 0.8059\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 13s 45ms/step - loss: 0.9405 - sparse_categorical_accuracy: 0.7513 - sparse_top_k_categorical_accuracy: 0.9071 - val_loss: 1.6460 - val_sparse_categorical_accuracy: 0.6362 - val_sparse_top_k_categorical_accuracy: 0.8090\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 13s 47ms/step - loss: 0.6993 - sparse_categorical_accuracy: 0.8177 - sparse_top_k_categorical_accuracy: 0.9444 - val_loss: 1.8733 - val_sparse_categorical_accuracy: 0.6394 - val_sparse_top_k_categorical_accuracy: 0.8108\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 13s 45ms/step - loss: 0.5242 - sparse_categorical_accuracy: 0.8672 - sparse_top_k_categorical_accuracy: 0.9676 - val_loss: 2.1061 - val_sparse_categorical_accuracy: 0.6260 - val_sparse_top_k_categorical_accuracy: 0.8059\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 12s 43ms/step - loss: 0.4121 - sparse_categorical_accuracy: 0.8958 - sparse_top_k_categorical_accuracy: 0.9800 - val_loss: 2.2867 - val_sparse_categorical_accuracy: 0.6331 - val_sparse_top_k_categorical_accuracy: 0.8108\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 12s 43ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.9144 - sparse_top_k_categorical_accuracy: 0.9891 - val_loss: 2.4688 - val_sparse_categorical_accuracy: 0.6064 - val_sparse_top_k_categorical_accuracy: 0.8108\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 17s 59ms/step - loss: 0.2969 - sparse_categorical_accuracy: 0.9269 - sparse_top_k_categorical_accuracy: 0.9922 - val_loss: 2.6198 - val_sparse_categorical_accuracy: 0.6256 - val_sparse_top_k_categorical_accuracy: 0.8054\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 13s 47ms/step - loss: 0.2629 - sparse_categorical_accuracy: 0.9367 - sparse_top_k_categorical_accuracy: 0.9939 - val_loss: 2.7039 - val_sparse_categorical_accuracy: 0.6287 - val_sparse_top_k_categorical_accuracy: 0.8063\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-defined `train_on_batch` method\n",
    "\n",
    "\n",
    "This pre-defined method allows fine-controlling to the training procedure for each batch without the callbacks, which is even more flexible than `fit` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,ds_train,ds_valid,epoches):\n",
    "\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        model.reset_metrics()\n",
    "        \n",
    "        # Reduce learning rate at the late stage of training.\n",
    "        if epoch == 5:\n",
    "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
    "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
    "        \n",
    "        for x, y in ds_train:\n",
    "            train_result = model.train_on_batch(x, y)\n",
    "\n",
    "        for x, y in ds_valid:\n",
    "            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n",
    "            \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch = \",epoch)\n",
    "            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n",
    "            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================20:43:14\n",
      "epoch =  1\n",
      "train: {'loss': 1.9161746501922607, 'sparse_categorical_accuracy': 0.5, 'sparse_top_k_categorical_accuracy': 0.7272727489471436}\n",
      "valid: {'loss': 1.653507113456726, 'sparse_categorical_accuracy': 0.5712377429008484, 'sparse_top_k_categorical_accuracy': 0.7635797262191772}\n",
      "\n",
      "================================================================================20:43:25\n",
      "epoch =  2\n",
      "train: {'loss': 1.4961708784103394, 'sparse_categorical_accuracy': 0.6363636255264282, 'sparse_top_k_categorical_accuracy': 0.8181818127632141}\n",
      "valid: {'loss': 1.5018855333328247, 'sparse_categorical_accuracy': 0.6117542386054993, 'sparse_top_k_categorical_accuracy': 0.7951914668083191}\n",
      "\n",
      "================================================================================20:43:33\n",
      "epoch =  3\n",
      "train: {'loss': 1.1854312419891357, 'sparse_categorical_accuracy': 0.7272727489471436, 'sparse_top_k_categorical_accuracy': 0.8181818127632141}\n",
      "valid: {'loss': 1.504996418952942, 'sparse_categorical_accuracy': 0.628227949142456, 'sparse_top_k_categorical_accuracy': 0.8076580762863159}\n",
      "\n",
      "================================================================================20:43:41\n",
      "epoch =  4\n",
      "train: {'loss': 0.8252235054969788, 'sparse_categorical_accuracy': 0.7272727489471436, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 1.647735357284546, 'sparse_categorical_accuracy': 0.6375778913497925, 'sparse_top_k_categorical_accuracy': 0.8116651773452759}\n",
      "\n",
      "Lowering optimizer Learning Rate...\n",
      "\n",
      "\n",
      "================================================================================20:43:52\n",
      "epoch =  5\n",
      "train: {'loss': 0.4746633470058441, 'sparse_categorical_accuracy': 0.8636363744735718, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 1.8316264152526855, 'sparse_categorical_accuracy': 0.6406945586204529, 'sparse_top_k_categorical_accuracy': 0.8076580762863159}\n",
      "\n",
      "================================================================================20:43:58\n",
      "epoch =  6\n",
      "train: {'loss': 0.3420068919658661, 'sparse_categorical_accuracy': 0.8636363744735718, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 1.9813252687454224, 'sparse_categorical_accuracy': 0.6406945586204529, 'sparse_top_k_categorical_accuracy': 0.8076580762863159}\n",
      "\n",
      "================================================================================20:44:06\n",
      "epoch =  7\n",
      "train: {'loss': 0.2556784749031067, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 2.1413238048553467, 'sparse_categorical_accuracy': 0.6402493119239807, 'sparse_top_k_categorical_accuracy': 0.8076580762863159}\n",
      "\n",
      "================================================================================20:44:14\n",
      "epoch =  8\n",
      "train: {'loss': 0.20543164014816284, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 2.2902212142944336, 'sparse_categorical_accuracy': 0.6380231380462646, 'sparse_top_k_categorical_accuracy': 0.8094390034675598}\n",
      "\n",
      "================================================================================20:44:22\n",
      "epoch =  9\n",
      "train: {'loss': 0.1718578040599823, 'sparse_categorical_accuracy': 0.9545454382896423, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 2.4297914505004883, 'sparse_categorical_accuracy': 0.6362422108650208, 'sparse_top_k_categorical_accuracy': 0.8125556707382202}\n",
      "\n",
      "================================================================================20:44:29\n",
      "epoch =  10\n",
      "train: {'loss': 0.14921307563781738, 'sparse_categorical_accuracy': 0.9545454382896423, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 2.558096170425415, 'sparse_categorical_accuracy': 0.6362422108650208, 'sparse_top_k_categorical_accuracy': 0.8112199306488037}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Customized Training Loop\n",
    "\n",
    "\n",
    "Re-compilation of the model is not required in the customized training loop, just back-propagate the iterative parameters through the optimizer according to the loss function, which gives us the highest flexibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================20:44:39\n",
      "Epoch=1,Loss:2.03962,Accuracy:0.467267871,Valid Loss:1.6862855,Valid Accuracy:0.55921638\n",
      "\n",
      "================================================================================20:44:45\n",
      "Epoch=2,Loss:1.50416291,Accuracy:0.606546402,Valid Loss:1.53696525,Valid Accuracy:0.605520904\n",
      "\n",
      "================================================================================20:44:51\n",
      "Epoch=3,Loss:1.22702539,Accuracy:0.670786,Valid Loss:1.54396808,Valid Accuracy:0.628673196\n",
      "\n",
      "================================================================================20:44:56\n",
      "Epoch=4,Loss:0.956361175,Accuracy:0.74704963,Valid Loss:1.70621467,Valid Accuracy:0.631344616\n",
      "\n",
      "================================================================================20:45:02\n",
      "Epoch=5,Loss:0.710851192,Accuracy:0.815408587,Valid Loss:1.93503666,Valid Accuracy:0.627337515\n",
      "\n",
      "================================================================================20:45:07\n",
      "Epoch=6,Loss:0.534460723,Accuracy:0.864840806,Valid Loss:2.15380549,Valid Accuracy:0.621549428\n",
      "\n",
      "================================================================================20:45:13\n",
      "Epoch=7,Loss:0.422752231,Accuracy:0.897906899,Valid Loss:2.35073638,Valid Accuracy:0.621104181\n",
      "\n",
      "================================================================================20:45:20\n",
      "Epoch=8,Loss:0.351828963,Accuracy:0.915497661,Valid Loss:2.54012132,Valid Accuracy:0.618878\n",
      "\n",
      "================================================================================20:45:27\n",
      "Epoch=9,Loss:0.304628968,Accuracy:0.926074386,Valid Loss:2.67512393,Valid Accuracy:0.61576134\n",
      "\n",
      "================================================================================20:45:34\n",
      "Epoch=10,Loss:0.271492958,Accuracy:0.932086408,Valid Loss:2.76720333,Valid Accuracy:0.616651833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
